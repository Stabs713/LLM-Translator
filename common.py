import os
import requests
from tqdm import tqdm
from dotenv import load_dotenv
import re

# –ù–∞—Å—Ç—Ä–æ–π–∫–∏
INPUT_DIR = "inputs"
OUTPUT_DIR = "outputs"

# –ó–∞—â–∏—â—ë–Ω–Ω—ã–µ –º–∞–∫—Ä–æ—Å—ã ‚Äî –ù–ï –ø–µ—Ä–µ–≤–æ–¥–∏—Ç—å
PROTECTED_MACROS = {
    # –ë–∞–∑–æ–≤—ã–µ –∫–æ–º–∞–Ω–¥—ã –¥–æ–∫—É–º–µ–Ω—Ç–∞
    'documentclass', 'usepackage', 'RequirePackage',

    # –ú–µ—Ç–∫–∏ –∏ —Å—Å—ã–ª–∫–∏
    'label', 'ref', 'eqref', 'pageref', 'autoref',

    # –ë–∏–±–ª–∏–æ–≥—Ä–∞—Ñ–∏—è
    'cite', 'bibitem', 'bibliographystyle', 'bibliography', 'nocite',

    # –ì—Ä–∞—Ñ–∏–∫–∞ –∏ —Ñ–∞–π–ª—ã
    'includegraphics', 'input', 'include', 'subfile',

    # –ì–∏–ø–µ—Ä—Å—Å—ã–ª–∫–∏
    'url', 'href', 'footnotemark', 'footnotetext',

    # –¢–∞–±–ª–∏—Ü—ã –∏ –º–∞—Ç—Ä–∏—Ü—ã
    'hline', 'cline', 'multicolumn', 'multirow', 'cellcolor',

    # –û—Ñ–æ—Ä–º–ª–µ–Ω–∏–µ
    'pagestyle', 'pagenumbering', 'thispagestyle',

    # –ú–∞—Ç–µ–º–∞—Ç–∏–∫–∞ (–∫–æ–º–∞–Ω–¥—ã, –∞ –Ω–µ —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ)
    'newcommand', 'renewcommand', 'DeclareMathOperator',

    # –ò–Ω–¥–µ–∫—Å –∏ –≥–ª–æ—Å—Å–∞—Ä–∏–∏
    'index', 'gls', 'glsadd', 'printglossary',

    # –ü—Ä–æ—á–∏–µ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ
    'begin', 'end',
    'addbibresource',
    'usetikzlibrary', 'usepgflibrary',
    'hypersetup',
    'def', 'let',
}

# –ú–∞–∫—Ä–æ—Å—ã, —á—å–∏ –∞—Ä–≥—É–º–µ–Ω—Ç—ã –ú–û–ñ–ù–û –ø–µ—Ä–µ–≤–æ–¥–∏—Ç—å
TRANSLATABLE_MACROS = {
    # –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞
    'section', 'subsection', 'subsubsection', 'paragraph', 'subparagraph',
    'chapter', 'part', 'title', 'author', 'date', 'affil',

    # –û—Ñ–æ—Ä–º–ª–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞
    'caption', 'shortcaption',
    'textbf', 'textit', 'emph', 'underline', 'texttt', 'textsf', 'textrm',
    'textsc', 'textsl', 'textsuperscript', 'textsubscript',

    # –°–ø–∏—Å–∫–∏ –∏ –∞–±–∑–∞—Ü—ã
    'item', 'footnote',

    # –ê–±—Å—Ç—Ä–∞–∫—Ç –∏ –±–ª–æ–∫–∏
    'abstract', 'keywords',

    # –¢–µ–æ—Ä–µ–º—ã –∏ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è
    'theorem', 'lemma', 'proposition', 'definition', 'corollary',
}

# –ó–∞—â–∏—â—ë–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è (–ù–ï –ø–µ—Ä–µ–≤–æ–¥–∏–º —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ)
PROTECTED_ENVIRONMENTS = {
    # –ú–∞—Ç–µ–º–∞—Ç–∏–∫–∞
    'equation', 'equation*', 'align', 'align*', 'gather', 'gather*',
    'multline', 'multline*', 'eqnarray', 'eqnarray*', 'displaymath',
    'math', '$',

    # –ö–æ–¥ –∏ verbatim
    'verbatim', 'lstlisting', 'minted', 'code', 'Verbatim',

    # –ü—Ä–æ—á–∏–µ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ
    'tikzpicture', 'asy', 'pspicture',
}

# –¢—Ä–∞–Ω—Å–ª–∏—Ä—É–µ–º—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è (–ø–µ—Ä–µ–≤–æ–¥–∏–º —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ)
TRANSLATABLE_ENVIRONMENTS = {
    'table', 'figure',
    'center', 'flushleft', 'flushright',
    'quote', 'quotation', 'verse',
    'itemize', 'enumerate', 'description',
    'tabular', 'tabularx', 'tabulary', 'longtable',
}

# –ì–ª–æ–±–∞–ª—å–Ω—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ
PROXY_API_URL = None
PROXY_API_KEY = None
CURRENT_MODEL = "gpt-4o"


def load_env_vars():
    global PROXY_API_URL, PROXY_API_KEY
    load_dotenv()
    PROXY_API_URL = os.getenv("PROXY_API_URL", "https://api.proxyapi.ru/openai/v1/chat/completions")
    PROXY_API_KEY = os.getenv("PROXY_API_KEY")
    if not PROXY_API_KEY:
        raise ValueError("‚ùå PROXY_API_KEY –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ .env. –î–æ–±–∞–≤—å—Ç–µ –µ–≥–æ.")


def set_current_model(model_name):
    global CURRENT_MODEL
    CURRENT_MODEL = model_name


def get_current_model():
    return CURRENT_MODEL


def chunk_text_by_sentences_safe(text, max_tokens=1500):
    """–†–∞–∑–±–∏–≤–∞–µ—Ç —Ç–µ–∫—Å—Ç –Ω–∞ —á–∞–Ω–∫–∏ –ø–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è–º —Å —É–ª—É—á—à–µ–Ω–Ω—ã–º regex"""
    if not text.strip():
        return [text]

    # –£–ª—É—á—à–µ–Ω–Ω—ã–π regex: —É—á–∏—Ç—ã–≤–∞–µ—Ç —Ü–∏—Ñ—Ä—ã –∏ —Å–∫–æ–±–∫–∏ –≤ –Ω–∞—á–∞–ª–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
    sentences = re.split(r'(?<=[.!?])\s+(?=[A-Z–ê-–Ø\d(])', text.strip())
    if not sentences:
        return [text]

    chunks = []
    current_chunk = []
    current_len = 0

    for sent in sentences:
        tokens = len(sent) // 4

        if not current_chunk:
            current_chunk = [sent]
            current_len = tokens
        elif current_len + tokens <= max_tokens:
            current_chunk.append(sent)
            current_len += tokens
        else:
            chunks.append(" ".join(current_chunk))
            current_chunk = [sent]
            current_len = tokens

    if current_chunk:
        chunks.append(" ".join(current_chunk))

    return chunks


def translate_chunk(text, retries=3):
    """–ü–µ—Ä–µ–≤–æ–¥–∏—Ç –æ–¥–∏–Ω —á–∞–Ω–∫ —Ç–µ–∫—Å—Ç–∞ —Å —É–ª—É—á—à–µ–Ω–Ω—ã–º –ø—Ä–æ–º–ø—Ç–æ–º"""

    # –ï—Å–ª–∏ —Ç–µ–∫—Å—Ç —Å–æ—Å—Ç–æ–∏—Ç —Ç–æ–ª—å–∫–æ –∏–∑ LaTeX –∫–æ–º–∞–Ω–¥ –∏ placeholder'–æ–≤, –Ω–µ –ø–µ—Ä–µ–≤–æ–¥–∏–º
    if re.fullmatch(r'[\s\\{}\[\]_^&$__PROTECTED_\d+__]+', text):
        return text

    prompt = f"""–ü–µ—Ä–µ–≤–µ–¥–∏ –≤–µ—Å—å –∞–Ω–≥–ª–∏–π—Å–∫–∏–π —Ç–µ–∫—Å—Ç –Ω–∞ —Ä—É—Å—Å–∫–∏–π. –ö–†–ò–¢–ò–ß–ï–°–ö–ò –í–ê–ñ–ù–û:

1. –ü–µ—Ä–µ–≤–æ–¥–∏ –ê–ë–°–û–õ–Æ–¢–ù–û –í–°–Å —á—Ç–æ —è–≤–ª—è–µ—Ç—Å—è —Ç–µ–∫—Å—Ç–æ–º (—Å–ª–æ–≤–∞, –∑–∞–≥–æ–ª–æ–≤–∫–∏, –ø–æ–¥–ø–∏—Å–∏, —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ —Ç–∞–±–ª–∏—Ü)
2. –ù–ï –¢–†–û–ì–ê–ô:
   - –ú–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ —Ñ–æ—Ä–º—É–ª—ã –∏ —Å–∏–º–≤–æ–ª—ã: $...$, $$...$$, \\[...\\], dXt, ¬µ, œÉ, Wt –∏ —Ç.–¥.
   - LaTeX –∫–æ–º–∞–Ω–¥—ã: \\section, \\caption, \\textbf, \\begin, \\end
   - –°—Ç—Ä—É–∫—Ç—É—Ä—É —Ç–∞–±–ª–∏—Ü: &, \\\\, \\hline
   - –ú–∞—Ä–∫–µ—Ä—ã __PROTECTED_N__
3. –ü–µ—Ä–µ–≤–æ–¥–∏ —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ –≤–Ω—É—Ç—Ä–∏ —Ñ–∏–≥—É—Ä–Ω—ã—Ö —Å–∫–æ–±–æ–∫: \\section{{Introduction}} ‚Üí \\section{{–í–≤–µ–¥–µ–Ω–∏–µ}}
4. –ü–µ—Ä–µ–≤–æ–¥–∏ —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ —Ç–∞–±–ª–∏—Ü: Parameter ‚Üí –ü–∞—Ä–∞–º–µ—Ç—Ä, Value ‚Üí –ó–Ω–∞—á–µ–Ω–∏–µ
5. –ù–ï –¥–æ–±–∞–≤–ª—è–π –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏, –ø–æ—è—Å–Ω–µ–Ω–∏—è, –Ω–µ –ø–∏—à–∏ "–í–æ—Ç –ø–µ—Ä–µ–≤–æ–¥"

–¢–µ–∫—Å—Ç –¥–ª—è –ø–µ—Ä–µ–≤–æ–¥–∞:
{text}

–ü–µ—Ä–µ–≤–µ–¥—ë–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç:"""

    headers = {
        "Content-Type": "application/json",
        "Authorization": f"Bearer {PROXY_API_KEY}"
    }

    payload = {
        "model": get_current_model(),
        "messages": [{"role": "user", "content": prompt}],
        "max_tokens": 3000,
        "temperature": 0.2,
        "top_p": 0.95
    }

    for attempt in range(retries):
        try:
            response = requests.post(PROXY_API_URL, json=payload, headers=headers, timeout=120)
            if response.status_code == 200:
                result = response.json().get("choices", [{}])[0].get("message", {}).get("content", "").strip()
                if result:
                    return result
        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∑–∞–ø—Ä–æ—Å–∞ (–ø–æ–ø—ã—Ç–∫–∞ {attempt+1}/{retries}): {e}")
            pass
        if attempt < retries - 1:
            import time
            time.sleep(1)
    return text


def test_model_connection(model_name):
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ –º–æ–¥–µ–ª–∏"""
    print(f"üåê –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ –º–æ–¥–µ–ª–∏: {model_name}")
    headers = {
        "Content-Type": "application/json",
        "Authorization": f"Bearer {PROXY_API_KEY}"
    }
    payload = {
        "model": model_name,
        "messages": [{"role": "user", "content": "ping"}],
        "max_tokens": 5
    }
    try:
        response = requests.post(PROXY_API_URL, json=payload, headers=headers, timeout=10)
        if response.status_code == 200:
            print(f"‚úÖ –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ –º–æ–¥–µ–ª–∏ {model_name} —É—Å–ø–µ—à–Ω–æ!")
            return True
        else:
            print(f"‚ùå –û—à–∏–±–∫–∞ HTTP {response.status_code}: {response.text}")
            return False
    except Exception as e:
        print(f"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–¥–∫–ª—é—á–∏—Ç—å—Å—è: {e}")
        return False


def get_files_list(directory):
    """–ü–æ–ª—É—á–∞–µ—Ç —Å–ø–∏—Å–æ–∫ —Ñ–∞–π–ª–æ–≤ –¥–ª—è –ø–µ—Ä–µ–≤–æ–¥–∞"""
    files = [f for f in os.listdir(directory) if f.lower().endswith(('.docx', '.tex', '.zip'))]
    return sorted(files)


def get_tex_files_list(directory):
    """–ü–æ–ª—É—á–∞–µ—Ç —Å–ø–∏—Å–æ–∫ .tex —Ñ–∞–π–ª–æ–≤"""
    files = [f for f in os.listdir(directory) if f.lower().endswith('.tex')]
    return sorted(files)


def select_file_by_number(total_count):
    """–í—ã–±–æ—Ä —Ñ–∞–π–ª–∞ –ø–æ –Ω–æ–º–µ—Ä—É"""
    while True:
        try:
            choice = int(input(f"\n–í—ã–±–µ—Ä–∏—Ç–µ –Ω–æ–º–µ—Ä —Ñ–∞–π–ª–∞ (1-{total_count}): ").strip())
            if 1 <= choice <= total_count:
                return choice
            else:
                print(f"‚ùå –ù–æ–º–µ—Ä –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –æ—Ç 1 –¥–æ {total_count}.")
        except ValueError:
            print("‚ùå –í–≤–µ–¥–∏—Ç–µ —á–∏—Å–ª–æ.")


def select_translation_model():
    """–í—ã–±–æ—Ä –º–æ–¥–µ–ª–∏ –ø–µ—Ä–µ–≤–æ–¥–∞"""
    model_names = ["gpt-4o", "gpt-4o-mini"]
    print("\n–î–æ—Å—Ç—É–ø–Ω—ã–µ –º–æ–¥–µ–ª–∏ –¥–ª—è –ø–µ—Ä–µ–≤–æ–¥–∞:")
    for i, name in enumerate(model_names, 1):
        print(f"  {i}. {name}")
    while True:
        try:
            choice = int(input(f"\n–í—ã–±–µ—Ä–∏—Ç–µ –º–æ–¥–µ–ª—å (1-{len(model_names)}): ").strip())
            if 1 <= choice <= len(model_names):
                selected = model_names[choice - 1]
                print(f"‚úÖ –í—ã–±—Ä–∞–Ω–∞ –º–æ–¥–µ–ª—å: {selected}")
                return selected
            else:
                print(f"‚ùå –ù–æ–º–µ—Ä –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –æ—Ç 1 –¥–æ {len(model_names)}.")
        except ValueError:
            print("‚ùå –í–≤–µ–¥–∏—Ç–µ —á–∏—Å–ª–æ.")
